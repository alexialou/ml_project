{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sdae\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32912, 188)\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, cost -4193432989203.013672\n",
      "Pre-training layer 0, epoch 1, cost -12611427789993.496094\n",
      "Pre-training layer 0, epoch 2, cost -21014111538091.757812\n",
      "Pre-training layer 0, epoch 3, cost -29287110122103.351562\n",
      "Pre-training layer 0, epoch 4, cost -37436583740611.882812\n",
      "Pre-training layer 0, epoch 5, cost -45645687527183.320312\n",
      "Pre-training layer 0, epoch 6, cost -53933427260590.882812\n",
      "Pre-training layer 0, epoch 7, cost -62331181870389.171875\n",
      "Pre-training layer 0, epoch 8, cost -70709428482895.828125\n",
      "Pre-training layer 0, epoch 9, cost -79103623235978.562500\n",
      "Pre-training layer 0, epoch 10, cost -87469663721778.625000\n",
      "Pre-training layer 0, epoch 11, cost -95814174785065.484375\n",
      "Pre-training layer 0, epoch 12, cost -104207632470114.140625\n",
      "Pre-training layer 0, epoch 13, cost -112669203014011.593750\n",
      "Pre-training layer 0, epoch 14, cost -121058134780962.812500\n",
      "Pre-training layer 1, epoch 0, cost 0.035577\n",
      "Pre-training layer 1, epoch 1, cost 0.000353\n",
      "Pre-training layer 1, epoch 2, cost 0.000211\n",
      "Pre-training layer 1, epoch 3, cost 0.000151\n",
      "Pre-training layer 1, epoch 4, cost 0.000118\n",
      "Pre-training layer 1, epoch 5, cost 0.000097\n",
      "Pre-training layer 1, epoch 6, cost 0.000083\n",
      "Pre-training layer 1, epoch 7, cost 0.000072\n",
      "Pre-training layer 1, epoch 8, cost 0.000064\n",
      "Pre-training layer 1, epoch 9, cost 0.000057\n",
      "Pre-training layer 1, epoch 10, cost 0.000052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ea4d292c18df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretraining_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorruption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorruption_levels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;31m#             c.append(pretraining_fns[i](index=batch_index,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#                                         corruption=corruption_levels[i],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run(datapath):\n",
    "    normed = pd.read_csv(datapath)\n",
    "    normed = normed.loc[normed['clerk_school'].notnull()]\n",
    "    normed = normed.drop(['judge'], axis=1)\n",
    "    normed['clerk_school'] = pd.Categorical(normed.clerk_school).codes\n",
    "    y_data = normed['clerk_school'].as_matrix()\n",
    "    X_data = normed.drop(['clerk_school'], axis=1).as_matrix()\n",
    "\n",
    "    uniq_sch = len(np.unique(y_data))\n",
    "    feature_num = X_data.shape[1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = theano.shared(X_train.astype('float64'))\n",
    "    y_train = theano.shared(y_train.astype('int32'))\n",
    "    X_val = theano.shared(X_val.astype('float64'))\n",
    "    y_val = theano.shared(y_val.astype('int32'))\n",
    "    X_test = theano.shared(X_test.astype('float64'))\n",
    "    y_test = theano.shared(y_test.astype('int32'))\n",
    "\n",
    "    datasets = np.array([(X_train, y_train), (X_val, y_val), (X_test, y_test)])\n",
    "\n",
    "\n",
    "    finetune_lr=0.1\n",
    "    pretraining_epochs=15\n",
    "    pretrain_lr=0.001,\n",
    "    training_epochs=500\n",
    "    batch_size=1\n",
    "\n",
    "    X_train = datasets[0][0]\n",
    "    n_train_batches = X_train.get_value(borrow=True).shape[0]\n",
    "    n_train_batches //= batch_size\n",
    "\n",
    "    numpy_rng = np.random.RandomState(89677)\n",
    "    encoder = sdae.SdA(numpy_rng=numpy_rng,\n",
    "                       n_ins=feature_num,\n",
    "                       hidden_layers_sizes=[1000, 1000, 1000],\n",
    "                       n_outs=uniq_sch)\n",
    "    pretraining_fns = encoder.pretraining_functions(train_set_x=X_train, batch_size=batch_size)\n",
    "    print('... pre-training the model')\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "    corruption_levels = [.1, .2, .3]\n",
    "    for i in range(encoder.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index, corruption=corruption_levels[i]))\n",
    "    #             c.append(pretraining_fns[i](index=batch_index,\n",
    "    #                                         corruption=corruption_levels[i],\n",
    "    #                                         lr=pretrain_lr))\n",
    "\n",
    "            print('Pre-training layer %i, epoch %d, cost %f' % (i, epoch, np.mean(c, dtype='float64')))\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('The pretraining code ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n",
    "\n",
    "    print('... getting the finetuning functions')\n",
    "    train_fn, validate_model, test_model = encoder.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "    print('... finetunning the model')\n",
    "        # early-stopping parameters\n",
    "    patience = 100 * n_train_batches  # look as this many examples regardless\n",
    "    patience_increase = 2.  # wait this much longer when a new best is\n",
    "                                # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                       # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                      # go through this many\n",
    "                                      # minibatche before checking the network\n",
    "                                      # on the validation set; in this case we\n",
    "                                      # check every epoch\n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = np.mean(validation_losses, dtype='float64')\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        # save best validation score and iteration number\n",
    "                        best_validation_loss = this_validation_loss\n",
    "                        best_iter = iter\n",
    "                        # test it on the test set\n",
    "                        test_losses = test_model()\n",
    "                        test_score = np.mean(test_losses, dtype='float64')\n",
    "                        print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                               'best model %f %%') %\n",
    "                              (epoch, minibatch_index + 1, n_train_batches, test_score * 100.))\n",
    "                if patience <= iter:\n",
    "                    done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print((\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'on iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print(('The training code ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('datapath') \n",
    "    args = parser.parse_args()\n",
    "    run(args.datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
